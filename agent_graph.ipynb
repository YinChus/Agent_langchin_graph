{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的六种设计模式之一：Chain\n",
    "Chain，也就是串联工作模式，并用这个agent来示例如何写议论文，从列提纲到写内容到润色修改，三个任务串联在一起，分别调用大模型，以实现更高的写作质量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import List, Dict, Callable\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_name = llm_name = \"qwen-plus\"\n",
    "\n",
    "def call_llm(user_prompt, system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_name,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(input: str, prompts: List[str]) -> str:\n",
    "    \"\"\"Chain multiple LLM calls sequentially, passing results between steps.\"\"\"\n",
    "    \n",
    "    result = input\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nStep {i}:\")\n",
    "        result = call_llm(f\"{prompt}\\n{result}\")\n",
    "        print(result)\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_processing_steps = [\n",
    "    \"\"\"\n",
    "    请根据以下主题撰写一篇议论文的大纲，\n",
    "    包含引言、三个到五个主要论点，以及结论。\n",
    "    每个论点附带简要的解释或论据，\n",
    "    主题如下：\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    请根据以下大纲撰写一篇完整的议论文，\n",
    "    要求语言流畅、逻辑清晰、论点充分，\n",
    "    每个段落围绕大纲内容展开，\n",
    "    字数为1000字左右。\n",
    "    大纲如下：\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    请对以下议论文进行全文润色，包括语法检查、用词优化和句式调整，\n",
    "    使文章语言自然流畅、逻辑清晰、表达生动而简洁，避免生硬和刻板的 AI 风格。\n",
    "    保持原文观点不变。\n",
    "    文章如下：\n",
    "    \"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain(\"人工智能时代更需要学习编程技术\",writing_processing_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的六种设计模式之二：Parallelization\n",
    "Parallelization，也就是并行工作模式，并用这个agent来示例如何将一个复杂任务分拆成多个关联不大的子任务，同时调用大模型接口，再汇总结果。\n",
    "mapreduce模式，是将一个复杂任务分拆成多个关联不大的子任务，然后调用大模型接口，再汇总结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import List, Dict, Callable\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = llm_name = \"qwen-plus\"\n",
    "\n",
    "def call_llm(user_prompt, system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_name,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多线程\n",
    "否则要等很久"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(inputs: List[str], n_workers: int = 3) -> List[str]:\n",
    "    \"\"\"Process multiple inputs concurrently with the same prompt.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(call_llm, prompt) for prompt in inputs]\n",
    "        return [f.result() for f in futures]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baijia=[\"儒家\",\"法家\",\"道家\",\"墨家\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(aspect, topic):\n",
    "    prompt = f\"\"\"\n",
    "    针对如下问题进行思考分析，需要一步步分析展示思考过程，并得出结论。\n",
    "    问题如下：{topic}。\n",
    "    你分析的框架和角度如下：{aspect}\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [gen_prompt(x, topic=\"to be or not to be\") for x in baijia]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_results = parallel(prompts, n_workers=len(prompts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in impact_results:\n",
    "    print('-------------')\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的六种设计模式之三：Routing\n",
    "Routing，也就是路由工作模式，Routing（路由） 根据输入内容的特征或条件，动态地将任务分配给最合适的子任务或大模型的一种工作流模式。它强调智能决策和灵活调度，适用于多样化输入和多策略处理的场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import List, Dict, Callable\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = llm_name = \"qwen-plus\"\n",
    "\n",
    "def call_llm(user_prompt, system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_name,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(text: str, tag: str) -> str:\n",
    "    match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)\n",
    "    return match.group(1) if match else \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_prompt = {\n",
    "    \"议论文\": \"\"\"\n",
    "    撰写一篇议论文，围绕以下主题展开明确的观点，并提供有力的论据支持。\n",
    "    要求：\n",
    "    1. **观点鲜明**：开篇提出明确的论点。\n",
    "    2. **逻辑严谨**：通过事实、数据、案例进行论证。\n",
    "    3. **结构清晰**：包括引言、论证、结论三部分。\n",
    "    4. **语言有说服力**：条理分明，语言简洁有力。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"散文\": \"\"\"\n",
    "    请写一篇散文，围绕以下主题抒发真挚细腻的情感，描绘生动的画面。\n",
    "    要求：\n",
    "    1. **情感真挚**：用细腻的文字表达内心感受。\n",
    "    2. **意境优美**：通过比喻、拟人等修辞，营造氛围。\n",
    "    3. **语言流畅**：语言自然，富有节奏感。\n",
    "    4. **不拘结构**：可以自由组织段落和内容。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"诗歌\": \"\"\"\n",
    "    请创作一首诗歌，围绕以下主题，用简洁生动的语言表达情感或描绘画面。\n",
    "    要求：\n",
    "    1. **意象丰富**：用生动的意象表现主题。\n",
    "    2. **语言凝练**：短句精练，富有节奏感。\n",
    "    3. **情感浓烈**：真挚自然地传达内心情感。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"小说\": \"\"\"\n",
    "    请创作一篇小说，围绕以下主题展开故事情节，塑造生动的角色，并营造引人入胜的氛围。\n",
    "    要求：\n",
    "    1. **情节完整**：包括开端、发展、高潮、结局。\n",
    "    2. **人物鲜明**：塑造富有个性和深度的角色。\n",
    "    3. **氛围生动**：通过细节描写和环境铺垫增强代入感。\n",
    "    4. **语言流畅**：叙述清晰，富有表现力。\n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(input: str, routes: Dict[str, str]) -> str:\n",
    "    \"\"\"Route input to specialized prompt using content classification.\"\"\"\n",
    "\n",
    "    selector_prompt = f\"\"\"\n",
    "    你是一个作家，请根据用户输入的主题，判断其需要的写作风格，并将内容路由到对应模块。\n",
    "    输入的主题是 {input}\n",
    "    路由的分类包括：\n",
    "    {list(routes.keys())}\n",
    "    你需要用简洁的语言归纳给出分类的理由，然后输出分类名称。都需要输出到 XML 格式标签中。\n",
    "\n",
    "    <reasoning>\n",
    "    分类的理由\n",
    "    </reasoning>\n",
    "\n",
    "    <selection>\n",
    "    分类名称\n",
    "    </selection>\n",
    "    \"\"\".strip()\n",
    "\n",
    "    route_response = call_llm(selector_prompt)\n",
    "    reasoning = extract_xml(route_response, 'reasoning')\n",
    "    route_key = extract_xml(route_response, 'selection').strip().lower()\n",
    "    print(\"分类的理由\")\n",
    "    print(reasoning)\n",
    "    print(f\"\\n分类名称: {route_key}\")\n",
    "\n",
    "    # Process input with selected specialized prompt\n",
    "    selected_prompt = routes[route_key]\n",
    "    return call_llm(f\"{selected_prompt}\\n要求字数五百字，主题是: {input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [\"为什么要学习编程\", \"天边的云\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for request in requests:\n",
    "    print(\"-\" * 40)\n",
    "    print(request)\n",
    "    print(\"-\" * 40)\n",
    "    response = route(request, writing_prompt)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的六种设计模式之四：Evaluator-optimizer\n",
    "Evaluator-optimizer，也就是评估优化工作模式，它是由一个大模型负责生成内容，另一个大模型负责进行评估和反馈。这样多轮迭代以优化大模型输出效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"qwen-plus\"\n",
    "\n",
    "def call_llm(user_prompt, system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_name,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(text: str, tag: str) -> str:\n",
    "    match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)\n",
    "    return match.group(1) if match else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_prompt = \"\"\"\n",
    "任务：\n",
    "根据提供的主题写一篇论证文章。请遵循以下结构和步骤，确保文章逻辑严密、有说服力。\n",
    "\n",
    "1. 引言部分\n",
    "开头简要介绍背景信息，说明该议题的重要性。清晰明确地提出文章的中心论点，即你要支持的观点。\n",
    "\n",
    "2. 主体部分\n",
    "主体部分应分为多个段落，每个段落围绕一个理由展开，支撑你的中心论点。\n",
    "\n",
    "3. 结论部分\n",
    "总结你的中心论点和主要支持理由，提醒读者文章的关键观点。\n",
    "\n",
    "4. 注意事项\n",
    "保持语言简洁明了，避免过于复杂或模糊的表达。\n",
    "确保论证严谨，避免情绪化的语言或空洞的说法。\n",
    "使用可靠的来源来支持论点，避免无根据的主张。\n",
    "字数要求1000字左右。\n",
    "\n",
    "如果有任何的反馈，你需要基于反馈来修改。\n",
    "\n",
    "你需要使用下面的XML标签来输出两部分内容。\n",
    "\n",
    "<thoughts>\n",
    "这里写你对主题的理解，对反馈内容的理解，以及如何来修改内容。\n",
    "</thoughts>\n",
    "\n",
    "<response>\n",
    "论文内容写在这里\n",
    "</response>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_prompt = \"\"\"\n",
    "请根据以下标准，评估输入的论证文章的质量和逻辑性：\n",
    "\n",
    "1. 明确性与中心论点：\n",
    "论证的中心论点是否清晰明确？是否能够轻松识别并理解论点？\n",
    "论证是否围绕一个可辨识的主题展开，避免陈述无争议的事实？\n",
    "\n",
    "2. 论证的事实性与前提：\n",
    "文章的前提是否真实且可靠？前提是否有明确的证据支持？\n",
    "是否存在任何事实错误或不准确的描述？\n",
    "\n",
    "3. 推理与逻辑性：\n",
    "论证的推理是否合理？是否能从前提出发，合逻辑地推导出结论？\n",
    "是否存在逻辑谬误，例如黑白谬误、以偏概全等？\n",
    "\n",
    "4. 证据支持与相关性：\n",
    "论证的证据是否充分且相关？每个支持理由是否有可靠的证据（例如专家意见、数据、案例）支持？\n",
    "是否有证据断层或刻意被遗漏？证据是否直接支持论点？\n",
    "\n",
    "5. 一致性与逻辑结构：\n",
    "论证是否存在自相矛盾的地方？前提和结论之间是否一致？\n",
    "论证的结构是否合理，段落之间是否有逻辑过渡，支持理由是否清晰组织？\n",
    "\n",
    "6. 批判性思维标准：\n",
    "清晰性：表述是否简洁明了？\n",
    "精准性：语言是否具体准确，避免模糊表述？\n",
    "相关性：所有内容是否与论点紧密相关？\n",
    "完整性：论证是否考虑了所有相关证据？\n",
    "公平性：辩论者是否公平处理反对意见，是否无偏见？\n",
    "\n",
    "7. 反对策略与对反对意见的回应：\n",
    "论证是否回应了反对意见？如果有，回应是否充分且合逻辑？\n",
    "论证是否忽略了相关证据，或者存在关键的反对意见没有被讨论？\n",
    "\n",
    "8. 结论的支持：\n",
    "论证的结论是否得到前提和证据的充分支持？结论是否合理、可靠？\n",
    "\n",
    "根据上述标准，总体评价该论证文章的质量，给出两部分反馈内容。\n",
    "根据文章的论证质量输出 **评分**，评分是 0 到 100 的区间，\n",
    "指出论证中的 **弱点**，并给出改进的建议。\n",
    "\n",
    "你需要使用下面的 XML 标签格式来输出评分和反馈：\n",
    "\n",
    "<evaluation>\n",
    "这里输出论文评分\n",
    "</evaluation>\n",
    "\n",
    "<feedback>\n",
    "这里输出反馈建议\n",
    "</feedback>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "AI的迅速发展会带来大量失业吗\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n{context}\\n文章主题如下:{task}\" if context else f\"{prompt}\\n文章主题如下:{task}\"\n",
    "    response = call_llm(full_prompt)\n",
    "    thoughts = extract_xml(response, \"thoughts\")\n",
    "    result = extract_xml(response, \"response\")\n",
    "\n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Thoughts:\\n{thoughts}\\n\")\n",
    "    print(f\"Generated:\\n{result}\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "\n",
    "    return thoughts, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n文章主题如下:{task}\\n要评估的内容如下: {content}\"\n",
    "    response = call_llm(full_prompt)\n",
    "    evaluation = extract_xml(response, \"evaluation\")\n",
    "    feedback = extract_xml(response, \"feedback\")\n",
    "\n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Score: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\")\n",
    "\n",
    "    return evaluation, feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "\n",
    "    # 生成初稿\n",
    "    thoughts, result = generate(generator_prompt, task)\n",
    "    memory.append(result)\n",
    "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "\n",
    "    # 进入迭代\n",
    "    while True:\n",
    "        # 评估文章内容\n",
    "        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n",
    "\n",
    "        # 评估通过则退出\n",
    "        if float(evaluation) >= 90:\n",
    "            return result, chain_of_thought\n",
    "\n",
    "        # 将之前内容和反馈进行拼接，作为背景知识\n",
    "        context = \"\\n\".join([\n",
    "            \"之前的写作内容如下：\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\n反馈如下：\\n{feedback}\"\n",
    "        ])\n",
    "\n",
    "        # 根据反馈重新生成\n",
    "        thoughts, result = generate(generator_prompt, task, context)\n",
    "        memory.append(result)\n",
    "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loop(task, evaluator_prompt, generator_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辩论场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_prompt = \"\"\"\n",
    "你是一个辩论场上的选手，你需要根据提供的正方观点进行论证。\n",
    "保持语言简洁明了，避免过于复杂或模糊的表达。\n",
    "确保论证严谨，避免情绪化的语言或空洞的说法。\n",
    "使用可靠的来源来支持论点，避免无根据的主张。\n",
    "\n",
    "如果辩论对方有任何的观点意见，你需要回应对方的观点意见并修改自己的论证。你的目标不是赢，而是探寻真理。\n",
    "\n",
    "你需要使用下面的XML标签来输出。\n",
    "\n",
    "<response>\n",
    "正方观点:\n",
    "正方论证过程:\n",
    "</response>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_prompt = \"\"\"\n",
    "你是一个辩论场上的选手，你需要根据提供的反方观点进行论证。\n",
    "保持语言简洁明了，避免过于复杂或模糊的表达。\n",
    "确保论证严谨，避免情感化的语言或空洞的说法。\n",
    "使用可靠的来源来支持论点，避免无根据的主张。\n",
    "\n",
    "如果辩论对方有任何的观点意见，你需要回应对方的观点意见并修改自己的论证。你的目标不是赢，而是探寻真理。\n",
    "\n",
    "你需要使用下面的XML标签来输出。\n",
    "\n",
    "<response>\n",
    "反方观点：\n",
    "反方论证过程：\n",
    "</response>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_task = \"正方观点如下: AI的迅速发展会带来大量失业\"\n",
    "neg_task = \"反方观点如下: AI的迅速发展不会带来大量失业\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_generate(prompt: str, task: str, context: str = \"\") -> str:\n",
    "    full_prompt = f\"{prompt}\\n{context}\\n{task}\" if context else f\"{prompt}\\n{task}\"\n",
    "    response = call_llm(full_prompt)\n",
    "    result = extract_xml(response, \"response\")\n",
    "\n",
    "    print(\"\\n=== POS START ===\")\n",
    "    print(f\"Generated:\\n{result}\")\n",
    "    print(\"=== POS END ===\\n\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_generate(prompt: str, task: str, context: str = \"\") -> str:\n",
    "    full_prompt = f\"{prompt}\\n{context}\\n{task}\" if context else f\"{prompt}\\n{task}\"\n",
    "    response = call_llm(full_prompt)\n",
    "    result = extract_xml(response, \"response\")\n",
    "\n",
    "    print(\"\\n=== NEG START ===\")\n",
    "    print(f\"Generated:\\n{result}\")\n",
    "    print(\"=== NEG END ===\\n\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(pos_task: str, neg_task: str, pos_prompt: str, neg_prompt: str) -> list:\n",
    "    memory = []\n",
    "    # 正方先输出\n",
    "    result = pos_generate(pos_prompt, pos_task)\n",
    "    memory.append(result)\n",
    "\n",
    "    # 进入迭代\n",
    "    for _ in range(3):\n",
    "        # 拼接背景内容\n",
    "        memory_string = \"\\n\".join(memory)\n",
    "        context = f\"双方论证历史信息如下：\\n{memory_string}\"\n",
    "        \n",
    "        # 反方输出\n",
    "        result = neg_generate(neg_prompt, context, neg_task)\n",
    "        memory.append(result)\n",
    "\n",
    "        # 拼接背景内容\n",
    "        memory_string = \"\\n\".join(memory)\n",
    "        context = f\"双方论证历史信息如下：\\n{memory_string}\"\n",
    "        \n",
    "        # 正方输出\n",
    "        result = pos_generate(pos_prompt, context, pos_task)\n",
    "        memory.append(result)\n",
    "\n",
    "    return memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loop(pos_task, neg_task, pos_prompt, neg_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orchestrator-Workers，它是由一个大模型作为管理协调者对复杂任务进行拆分，拆分成子任务以及任务说明，再分发给其它大模型进行下一步执行，它适合于无法在事前确定子任务个数的情况。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"qwen-plus\"\n",
    "\n",
    "def call_llm(user_prompt, system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_name,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml(text: str, tag: str) -> str:\n",
    "    match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)\n",
    "    return match.group(1) if match else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tasks(xml_string):\n",
    "    pattern = r'<task>(.*?)</task>'\n",
    "    tasks = re.findall(pattern, xml_string, re.DOTALL)\n",
    "    return tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleOrchestrator:\n",
    "    \"\"\"Break down tasks and run them in parallel using worker LLMs.\"\"\"\n",
    "    \n",
    "    def __init__(self, orchestrator_prompt: str, worker_prompt: str):\n",
    "        \"\"\"Initialize with prompt templates.\"\"\"\n",
    "        self.orchestrator_prompt = orchestrator_prompt\n",
    "        self.worker_prompt = worker_prompt\n",
    "\n",
    "    def _format_prompt(self, template: str, **kwargs):\n",
    "        \"\"\"Format a prompt template with variables.\"\"\"\n",
    "        try:\n",
    "            return template.format(**kwargs)\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing required prompt variable: {e}\")\n",
    "\n",
    "    def process(self, task: str, context=None):\n",
    "        \"\"\"Process task by breaking it down and running subtasks in parallel.\"\"\"\n",
    "        context = context or {}\n",
    "\n",
    "        # Step 1: Get orchestrator response\n",
    "        orchestrator_input = self._format_prompt(\n",
    "            self.orchestrator_prompt,\n",
    "            task=task\n",
    "        )\n",
    "        orchestrator_response = call_llm(orchestrator_input)\n",
    "\n",
    "        # Parse orchestrator response\n",
    "        analysis = extract_xml(orchestrator_response, \"analysis\")\n",
    "        tasks_xml = extract_xml(orchestrator_response, \"tasks\")\n",
    "        tasks = extract_tasks(tasks_xml)\n",
    "\n",
    "        print(\"\\n=== ORCHESTRATOR OUTPUT ===\")\n",
    "        print(f\"\\nANALYSIS:\\n{analysis}\")\n",
    "        print(f\"\\nTASKS:\\n{tasks}\")\n",
    "\n",
    "        # Step 2: Process each task\n",
    "        worker_results = []\n",
    "        for task_info in tasks:\n",
    "            worker_input = self._format_prompt(\n",
    "                self.worker_prompt,\n",
    "                original_task=task,\n",
    "                task_description=task_info\n",
    "            )\n",
    "            worker_response = call_llm(worker_input)\n",
    "            result = extract_xml(worker_response, \"response\")\n",
    "\n",
    "            worker_results.append({\n",
    "                \"description\": task_info,\n",
    "                \"result\": result\n",
    "            })\n",
    "\n",
    "            print(f\"\\n=== WORKER RESULT ({task_info}) ===\\n{result}\\n\")\n",
    "\n",
    "        return {\n",
    "            \"analysis\": analysis,\n",
    "            \"worker_results\": worker_results,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "你需要分析下面的问题，并将其分拆成几个不同的立场进行多视角分析。\n",
    "\n",
    "需要分析的问题如下：{task}\n",
    "\n",
    "返回格式输出示例如下：\n",
    "\n",
    "<analysis>在这里解释你对需要分析的问题的理解。</analysis>\n",
    "\n",
    "<tasks>\n",
    "    <task>在这里输出视角1</task>\n",
    "    <task>在这里输出视角2</task>\n",
    "</tasks>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKER_PROMPT = \"\"\"\n",
    "基于如下要求进行回答：\n",
    "你需要分析的问题如下：{original_task}\n",
    "你的分析视角如下：{task_description}\n",
    "\n",
    "返回格式示例如下：\n",
    "\n",
    "<response>\n",
    "在这里输出你的回答内容\n",
    "</response>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = FlexibleOrchestrator(\n",
    "    orchestrator_prompt=ORCHESTRATOR_PROMPT,\n",
    "    worker_prompt=WORKER_PROMPT,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = orchestrator.process(\n",
    "    task=\"AI时代需要学习编程\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的六种设计模式之六，ReAct Agent，它适合于开放性问题无法事先定义工作流，大模型自行规划执行条件和步骤，再根据外部反馈进行下一步行动，最终完成任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a ReAct (Reasoning and Acting) agent that follows a loop of Thought, Action, PAUSE, and Observation to solve problems.\n",
    "\n",
    "Workflow:\n",
    "1. Thought: Describe your reasoning or plan for solving the problem.\n",
    "2. Action: Execute an appropriate action based on your reasoning. The available actions are listed below.\n",
    "3. PAUSE: Indicate that you are pausing to observe the result of the action. Stop output anything while pause.\n",
    "4. Observation: Analyze the result of the action and incorporate it into your reasoning.\n",
    "\n",
    "At the end of the loop, provide a final Answer based on the information gathered.\n",
    "\n",
    "Your available actions are:\n",
    "  \n",
    "  calculate:\n",
    "    e.g. calculate: 5*7/4\n",
    "    run a calculation and returns the number using python so be sure to use floating point syntax if needed.\n",
    "\n",
    "  planet_mass:\n",
    "    e.g. planet_mass: Mars\n",
    "    returns the mass of the planet in the solar system\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(what):\n",
    "    return eval(what)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planet_mass(planet):\n",
    "    masses = {\n",
    "        \"Mercury\": 0.3301,\n",
    "        \"Venus\": 4.8557,\n",
    "        \"Earth\": 5.972,\n",
    "        \"Mars\": 0.6418,\n",
    "        \"Jupiter\": 1898.2,\n",
    "        \"Saturn\": 568.34,\n",
    "        \"Uranus\": 86.82,\n",
    "        \"Neptune\": 102.4,\n",
    "    }\n",
    "    return f\"{planet} has a mass of {masses[planet]} × 10^24 kg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "know_actions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"planet_mass\": planet_mass\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(system_prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(\"----------------------\")\n",
    "        print(f\"step: {i}\")\n",
    "        print(result)\n",
    "        action_re = re.compile(r\"^Action:\\s*(\\w+):\\s*(.*)$\")\n",
    "        actions = [action_re.match(line) for line in result.split(\"\\n\") if action_re.match(line)]\n",
    "        if actions:\n",
    "            print(\"\\nlet's do something\")\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in know_actions:\n",
    "                raise Exception(f\"Unknown action: {action}\")\n",
    "            print(f\"\\nRunning {action}({action_input})\")\n",
    "            observation = know_actions[action](action_input)\n",
    "            print(f\"\\nObservation: {observation}\")\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            return\n",
    "\n",
    "question = \"what is the combined mass of earth and jupiter\"\n",
    "query(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体的六种设计模式之框架篇：用LangGraph实现chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一部分：https://www.bilibili.com/video/BV1FrwReHEo2?spm_id_from=333.788.videopod.sections&vd_source=680c14d8fc47569fbccdd26dfabf6b28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(\n",
    "#     model='deepseek-chat',\n",
    "#     api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "#     base_url='https://api.deepseek.com'\n",
    "#     max_tokens=1024\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    outline: str\n",
    "    paper: str\n",
    "    final_paper: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_processing_steps = [\n",
    "    \"\"\"\n",
    "    请根据以下主题撰写一篇议论文的大纲，\n",
    "    包含引言、三到五个主要论点，以及结论。\n",
    "    每个论点附带简要的解释或论据，\n",
    "    主题如下：\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    请根据以下大纲撰写一篇完整的议论文，\n",
    "    要求语言流畅，逻辑清晰，论点充分，\n",
    "    每个段落围绕大纲内容展开，\n",
    "    字数为1000字左右。\n",
    "    大纲如下：\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    请对以下论文进行全文润色，包括语法检查、用词优化和句式调整，\n",
    "    使文章语言自然流畅，逻辑清晰，表达生动而简洁，\n",
    "    避免生硬和刻板的AI风格。保持原文观点不变。\n",
    "    文章如下：\n",
    "    \"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def generate_outline(state: State):\n",
    "    prompt = writing_processing_steps[0] + state[\"topic\"]\n",
    "    msg = llm.invoke(prompt)\n",
    "    return {\"outline\": msg.content}\n",
    "\n",
    "def generate_paper(state: State):\n",
    "    prompt = writing_processing_steps[1] + state[\"outline\"]\n",
    "    msg = llm.invoke(prompt)\n",
    "    return {\"paper\": msg.content}\n",
    "\n",
    "def polish_paper(state: State):\n",
    "    prompt = writing_processing_steps[2] + state[\"paper\"]\n",
    "    msg = llm.invoke(prompt)\n",
    "    return {\"final_paper\": msg.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"generate_outline\", generate_outline)\n",
    "workflow.add_node(\"generate_paper\", generate_paper)\n",
    "workflow.add_node(\"polish_paper\", polish_paper)\n",
    "workflow.add_edge(START, \"generate_outline\")\n",
    "workflow.add_edge(\"generate_outline\", \"generate_paper\")\n",
    "workflow.add_edge(\"generate_paper\", \"polish_paper\")\n",
    "workflow.add_edge(\"polish_paper\", END)\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({\"topic\": \"人工智能时代更要学习编程\"})\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二部分：https://www.bilibili.com/video/BV1h6AtecE8P?spm_id_from=333.788.videopod.sections&vd_source=680c14d8fc47569fbccdd26dfabf6b28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    part0: str\n",
    "    part1: str\n",
    "    part2: str\n",
    "    part3: str\n",
    "    combined_output: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baijia = [\n",
    "    \"儒家\",\n",
    "    \"法家\",\n",
    "    \"道家\",\n",
    "    \"墨家\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prompt = \"\"\"\n",
    "针对如下问题进行思考分析，需要一步步分析展示思考过程，并得出结论。\n",
    "问题如下：{topic}。\n",
    "你分析的框架和角度如下：{aspect}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def call_llm_0(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    aspect = baijia[0]\n",
    "    prompt = meta_prompt.format(topic=topic, aspect=aspect)\n",
    "    msg = llm.invoke(prompt)\n",
    "    return {\"part0\": msg.content}\n",
    "\n",
    "def call_llm_1(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    aspect = baijia[1]\n",
    "    prompt = meta_prompt.format(topic=topic, aspect=aspect)\n",
    "    msg = llm.invoke(prompt)\n",
    "    return {\"part1\": msg.content}\n",
    "def call_llm_2(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    aspect = baijia[2]\n",
    "    prompt = meta_prompt.format(topic=topic, aspect=aspect)\n",
    "    msg = llm.invoke(prompt)\n",
    "    return {\"part2\": msg.content}\n",
    "\n",
    "def call_llm_3(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    aspect = baijia[3]\n",
    "    prompt = meta_prompt.format(topic=topic, aspect=aspect)\n",
    "    msg = llm.invoke(prompt)\n",
    "    return {\"part3\": msg.content}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow\n",
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "parallel_builder.add_node(\"call_llm_0\", call_llm_0)\n",
    "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
    "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
    "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
    "parallel_builder.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "parallel_builder.add_edge(START, \"call_llm_0\")\n",
    "parallel_builder.add_edge(START, \"call_llm_1\")\n",
    "parallel_builder.add_edge(START, \"call_llm_2\")\n",
    "parallel_builder.add_edge(START, \"call_llm_3\")\n",
    "parallel_builder.add_edge(\"call_llm_0\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"aggregator\", END)\n",
    "\n",
    "parallel_workflow = parallel_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({\"topic\": \"to be or not be\"})\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三部分：https://www.bilibili.com/video/BV1X3PYeEEoK?spm_id_from=333.788.videopod.sections&vd_source=680c14d8fc47569fbccdd26dfabf6b28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_prompt = {\n",
    "    \"议论文\": \"\"\"\n",
    "    撰写一篇议论文，围绕以下主题展开明确的观点，并提供有力的论据支持。\n",
    "    要求：\n",
    "    1. **观点鲜明**：开篇提出明确的论点。\n",
    "    2. **逻辑严谨**：通过事实、数据、案例进行论证。\n",
    "    3. **结构清晰**：包括引言、论证、结论三部分。\n",
    "    4. **语言有说服力**：条理分明，语言简洁有力。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"散文\": \"\"\"\n",
    "    请写一篇散文，围绕以下主题抒发真挚细腻的情感，描绘生动的画面。\n",
    "    要求：\n",
    "    1. **情感真挚**：用细腻的文字表达内心感受。\n",
    "    2. **意境优美**：通过比喻、拟人等修辞，营造氛围。\n",
    "    3. **语言流畅**：语言自然，富有节奏感。\n",
    "    4. **不拘结构**：可以自由组织段落和内容。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"诗歌\": \"\"\"\n",
    "    请创作一首诗歌，围绕以下主题，用简洁生动的语言表达情感或描绘画面。\n",
    "    要求：\n",
    "    1. **意象丰富**：用生动的意象表现主题。\n",
    "    2. **语言凝练**：短句精练，富有节奏感。\n",
    "    3. **情感浓烈**：真挚自然地传达内心情感。\n",
    "    \"\"\",\n",
    "    \n",
    "    \"小说\": \"\"\"\n",
    "    请创作一篇小说，围绕以下主题展开故事情节，塑造生动的角色，并营造引人入胜的氛围。\n",
    "    要求：\n",
    "    1. **情节完整**：包括开端、发展、高潮、结局。\n",
    "    2. **人物鲜明**：塑造富有个性和深度的角色。\n",
    "    3. **氛围生动**：通过细节描写和环境铺垫增强代入感。\n",
    "    4. **语言流畅**：叙述清晰，富有表现力。\n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal['论文', '散文', '诗歌', '小说'] = Field(\n",
    "        default=None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: str\n",
    "\n",
    "def llm_call_1(state: State):\n",
    "    prompt = f\"{writing_prompt['议论文']} \\n主题如下:{state['input']}\"\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "def llm_call_2(state: State):\n",
    "    prompt = f\"{writing_prompt['散文']} \\n主题如下:{state['input']}\"\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "def llm_call_3(state: State):\n",
    "    prompt = f\"{writing_prompt['诗歌']} \\n主题如下:{state['input']}\"\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "def llm_call_4(state: State):\n",
    "    prompt = f\"{writing_prompt['小说']} \\n主题如下:{state['input']}\"\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"output\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_router(state: State):\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"根据用户输入的主题，判断其需要的写作风格，并将内容路由到对应模块。\"\n",
    "            ),\n",
    "            HumanMessage(content=state[\"input\"]),\n",
    "        ]\n",
    "    )\n",
    "    return {\"decision\": decision.step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_decision(state: State):\n",
    "    if state[\"decision\"] == \"议论文\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"散文\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"诗歌\":\n",
    "        return \"llm_call_3\"\n",
    "    elif state[\"decision\"] == \"小说\":\n",
    "        return \"llm_call_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_4\", llm_call_4)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "router_builder.add_edge(START, \"llm_call_router\")\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "        \"llm_call_4\": \"llm_call_4\",\n",
    "    },\n",
    ")\n",
    "\n",
    "router_builder.add_edge(\"llm_call_1\", END)\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "router_builder.add_edge(\"llm_call_4\", END)\n",
    "\n",
    "# Compile workflow\n",
    "router_workflow = router_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = router_workflow.invoke({\"input\": \"天边的云\"})\n",
    "print(state['decision'])\n",
    "print(state[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四部分：https://www.bilibili.com/video/BV1n79iY1Eg8?spm_id_from=333.788.videopod.sections&vd_source=680c14d8fc47569fbccdd26dfabf6b28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    paper: str\n",
    "    feedback: str\n",
    "    good_or_not: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Literal\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"合格\", \"不合格\"] = Field(\n",
    "        description=\"判断文章的逻辑性是否合格\",\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"根据文章的论证质量，给出修改建议\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = llm.with_structured_output(Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_generator(state: State):\n",
    "    if state.get(\"feedback\"):\n",
    "        prompt = f\"\"\"根据提供的主题写一篇论证文章。确保文章逻辑严密、有说服力。\n",
    "主题为：{state['topic']}。\n",
    "你同时需要考虑如下的修改建议：{state['feedback']}\"\"\"\n",
    "        msg = llm.invoke(prompt)\n",
    "    else:\n",
    "        prompt = f\"\"\"根据提供的主题写一篇论证文章。确保文章逻辑严密、有说服力。\n",
    "主题为：{state['topic']}。\"\"\"\n",
    "        msg = llm.invoke(prompt)\n",
    "\n",
    "    count = state['count'] + 1\n",
    "    return {\"paper\": msg.content, \"count\": count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_evaluator(state: State):\n",
    "    prompt = f\"\"\"根据提供的论证文章，判断是否逻辑严密、有说服力。\n",
    "文章如下：{state['paper']}\"\"\"\n",
    "    result = evaluator.invoke(prompt)\n",
    "    return {\"good_or_not\": result.grade, \"feedback\": result.feedback}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_paper(state: State):\n",
    "    if state[\"good_or_not\"] == \"合格\":\n",
    "        return \"Accepted\"\n",
    "    elif state['count'] >= 2:\n",
    "        return \"Accepted\"\n",
    "    elif state[\"good_or_not\"] == \"不合格\":\n",
    "        return \"Rejected and Feedback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow\n",
    "optimizer_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "optimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\n",
    "optimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "optimizer_builder.add_edge(START, \"llm_call_generator\")\n",
    "optimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\n",
    "optimizer_builder.add_conditional_edges(\n",
    "    \"llm_call_evaluator\",\n",
    "    route_paper,\n",
    "    {\n",
    "        \"Accepted\": END,\n",
    "        \"Rejected and Feedback\": \"llm_call_generator\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile the workflow\n",
    "optimizer_workflow = optimizer_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = optimizer_workflow.invoke({\"topic\": \"AI的迅速发展会带来大量失业吗\", 'count': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state[\"paper\"])\n",
    "print(state[\"good_or_not\"])\n",
    "print(state[\"feedback\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第五部分：https://www.bilibili.com/video/BV1rVQgYvE6i?spm_id_from=333.788.videopod.sections&vd_source=680c14d8fc47569fbccdd26dfabf6b28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"章节标题\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"章节概述\",\n",
    "    )\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"论证文章的各个子章节\",\n",
    "    )\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = planner.invoke(\"论证主题：AI时代需要学习编程，将这个论证分拆成几个不同的立场进行多视角分析\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State):\n",
    "    # Generate queries\n",
    "    output = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"你需要分析论证某个主题，并将其分拆成几个不同的立场进行多视角分析。\"),\n",
    "            HumanMessage(content=f\"论证主题：{state['topic']}\")\n",
    "        ]\n",
    "    )\n",
    "    return {\"sections\": output.sections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state: WorkerState):\n",
    "    # Generate section(content) str | list[str | dict]\n",
    "    output = llm.invoke(\n",
    "        content=[\n",
    "            SystemMessage(\n",
    "                content=\"根据提供的章节标题和章节概述，完成论证文章的其中一个章节\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"章节标题为：{state['section'].name} 章节概述为：{state['section'].description}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [output.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesizer(state: State):\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "state = orchestrator_worker.invoke({\"topic\": \"AI时代需要学习编程\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"sections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第六部分:https://www.bilibili.com/video/BV1xqXGYDELg?spm_id_from=333.788.videopod.sections&vd_source=680c14d8fc47569fbccdd26dfabf6b28\n",
    "\n",
    "智能体的六种设计模式之框架篇：用LangGraph实现ReAct Agent\n",
    "1. 概述：\n",
    "\n",
    "ReAct Agent是一种基于LangChain的AI代理设计模式，它通过结合ReAct和LangChain的Graph模块，实现了一个智能体，该智能体可以执行多个任务，并在执行过程中进行迭代优化。\n",
    "\n",
    "2. ReAct Agent的组成：\n",
    "\n",
    "ReAct Agent由三个部分组成：\n",
    "\n",
    "- 智能体：一个AI代理，它包含一个模型和一组工具。\n",
    "- 任务：一个描述智能体要执行的任务的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-max\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first float\n",
    "        b: second float\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def planet_mass(planet):\n",
    "    \"\"\"Return the mass of a planet, args: planet name(capitalized), e.g. Mercury\"\"\"\n",
    "    masses = {\n",
    "        \"Mercury\": 0.3301,\n",
    "        \"Venus\": 4.8557,\n",
    "        \"Earth\": 5.972,\n",
    "        \"Mars\": 0.6418,\n",
    "        \"Jupiter\": 1898.2,\n",
    "        \"Saturn\": 568.34,\n",
    "        \"Uranus\": 86.82,\n",
    "        \"Neptune\": 102.4\n",
    "    }\n",
    "    return f\"{planet} has a mass of {masses[planet]} 10^24 kg\"\n",
    "\n",
    "tools = [add, planet_mass]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(content=\"You are a helpful assistant.\")\n",
    "                ] + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "# def tool_node(state: MessagesState):\n",
    "#     result = []\n",
    "#     for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "#         tool = tools_by_name[tool_call[\"name\"]]\n",
    "#         observation = tool.invoke(tool_call[\"args\"])\n",
    "#         result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "#     return {\"messages\": result}\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    \n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"Action\": \"tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "agent_builder.add_edge(\"tools\", \"llm_call\")\n",
    "agent_builder.agent_builder.complile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "messages = [HumanMessage(content=\"what is the combined mass of earth and jupiter\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['message']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
